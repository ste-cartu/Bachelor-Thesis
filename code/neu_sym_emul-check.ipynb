{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <module 'numpy' from '/opt/homebrew/opt/python@3.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/__init__.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Lux, Optimisers, Random, Statistics, Zygote\n",
    "using CairoMakie, MakiePublication\n",
    "using PyCall, NPZ\n",
    "using Flux: params\n",
    "\n",
    "np = pyimport(\"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nk = 100\n",
    "nz = 20\n",
    "nc = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100×2000 Matrix{Float64}:\n",
       " 0.0055452   0.00420238  0.00320454  …  2.09809e13  1.91184e13  1.74928e13\n",
       " 0.00618531  0.00468762  0.00357466     2.96631e13  2.70296e13  2.47311e13\n",
       " 0.00687286  0.00520882  0.00397221     4.14293e13  3.77509e13  3.45403e13\n",
       " 0.00760954  0.00576726  0.00439818     5.72083e13  5.21285e13  4.76948e13\n",
       " 0.00839705  0.00636424  0.00485354     7.81614e13  7.12206e13  6.51626e13\n",
       " 0.00923708  0.00700104  0.00533928  …  1.05732e14  9.63419e13  8.81467e13\n",
       " 0.0101313   0.00767895  0.00585638     1.41696e14  1.29112e14  1.18129e14\n",
       " 0.0110815   0.00839924  0.00640582     1.88234e14  1.71516e14  1.56924e14\n",
       " 0.0120892   0.00916321  0.00698857     2.47997e14  2.2597e14   2.06745e14\n",
       " 0.0131563   0.00997214  0.00760562     3.24192e14  2.95396e14  2.70263e14\n",
       " 0.00545367  0.00413755  0.00315819  …  1.96395e13  1.79061e13  1.63924e13\n",
       " 0.00608319  0.00461532  0.003523       2.77421e13  2.52933e13  2.31548e13\n",
       " 0.00675936  0.0051285   0.00391486     3.87138e13  3.52962e13  3.23118e13\n",
       " ⋮                                   ⋱                          \n",
       " 0.0106213   0.00813039  0.00625569     1.32439e14  1.21191e14  1.11334e14\n",
       " 0.0115587   0.00884815  0.0068081      1.72417e14  1.57773e14  1.44941e14\n",
       " 0.00479726  0.00367588  0.00283083  …  1.0863e13   9.9459e12   9.1418e12\n",
       " 0.00535091  0.00410033  0.00315785     1.52576e13  1.39695e13  1.284e13\n",
       " 0.00594559  0.00455623  0.00350911     2.11774e13  1.93894e13  1.78217e13\n",
       " 0.00658275  0.00504472  0.00388549     2.90709e13  2.66164e13  2.44643e13\n",
       " 0.00726387  0.00556691  0.00428783     3.94966e13  3.61618e13  3.3238e13\n",
       " 0.00799042  0.00612393  0.00471702  …  5.31448e13  4.86577e13  4.47235e13\n",
       " 0.00876385  0.00671691  0.00517391     7.0864e13   6.48809e13  5.96351e13\n",
       " 0.00958563  0.00734696  0.00565938     9.36894e13  8.57792e13  7.88438e13\n",
       " 0.0104572   0.00801522  0.00617428     1.22877e14  1.12502e14  1.03407e14\n",
       " 0.0113801   0.0087228   0.00671949     1.59943e14  1.46439e14  1.346e14"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = npzread(\"../files/input_neu_sym_Vh_100.npy\")\n",
    "output = npzread(\"../files/output_neu_sym_Vh_100.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000×100 Matrix{Float64}:\n",
       " 1.36948e-17  1.0803e-17   8.59351e-18  …  4.88479e-18  4.08245e-18\n",
       " 1.0223e-17   8.06444e-18  6.41519e-18     3.68234e-18  3.07757e-18\n",
       " 7.64307e-18  6.02938e-18  4.7964e-18      2.77587e-18  2.32002e-18\n",
       " 5.77704e-18  4.55742e-18  3.62551e-18     2.11445e-18  1.76724e-18\n",
       " 4.42753e-18  3.49287e-18  2.77869e-18     1.63244e-18  1.36439e-18\n",
       " 3.43896e-18  2.71303e-18  2.15834e-18  …  1.27668e-18  1.06706e-18\n",
       " 2.70169e-18  2.13143e-18  1.69566e-18     1.00935e-18  8.43637e-19\n",
       " 2.14122e-18  1.68929e-18  1.34394e-18     8.04628e-19  6.72531e-19\n",
       " 1.70721e-18  1.3469e-18   1.07156e-18     6.44954e-19  5.39076e-19\n",
       " 1.36532e-18  1.07718e-18  8.56982e-19     5.18299e-19  4.33216e-19\n",
       " 1.09175e-18  8.61354e-19  6.85286e-19  …  4.16288e-19  3.47953e-19\n",
       " 8.69763e-19  6.86221e-19  5.45957e-19     3.32988e-19  2.78329e-19\n",
       " 6.87339e-19  5.42299e-19  4.31457e-19     2.64125e-19  2.20771e-19\n",
       " ⋮                                      ⋱               \n",
       " 0.164454     0.164446     0.164439        0.173653     0.173598\n",
       " 0.1406       0.140592     0.140585        0.149567     0.149517\n",
       " 0.121508     0.1215       0.121493     …  0.130168     0.130124\n",
       " 0.106011     0.106003     0.105996        0.114326     0.114286\n",
       " 0.093272     0.0932645    0.0932577       0.101226     0.101191\n",
       " 0.0826812    0.0826737    0.082667        0.0902731    0.0902415\n",
       " 0.0737855    0.0737782    0.0737717       0.0810226    0.0809941\n",
       " 0.0662446    0.0662374    0.066231     …  0.0731394    0.0731137\n",
       " 0.0597984    0.0597914    0.0597851       0.0663664    0.0663431\n",
       " 0.0542461    0.0542393    0.0542331       0.0605041    0.0604829\n",
       " 0.0494306    0.0494239    0.0494179       0.0553958    0.0553765\n",
       " 0.0452275    0.0452209    0.0452151       0.0509171    0.0508994"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = input'\n",
    "y = output'\n",
    "\n",
    "# standardizzazione y\n",
    "y = (y .- minimum(y, dims=1)) ./ (maximum(y, dims=1) .- minimum(y, dims=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: (2, 100)\ty: (2000, 100)"
     ]
    }
   ],
   "source": [
    "print(\"x: \", size(x), '\\t', \"y: \", size(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "    layer_1 = Dense(2 => 64, tanh_fast),  \u001b[90m# 192 parameters\u001b[39m\n",
       "    layer_2 = Dense(64 => 64, tanh_fast),  \u001b[90m# 4_160 parameters\u001b[39m\n",
       "    layer_3 = Dense(64 => 64, tanh_fast),  \u001b[90m# 4_160 parameters\u001b[39m\n",
       "    layer_4 = Dense(64 => 64, tanh_fast),  \u001b[90m# 4_160 parameters\u001b[39m\n",
       "    layer_5 = Dense(64 => 64, tanh_fast),  \u001b[90m# 4_160 parameters\u001b[39m\n",
       "    layer_6 = Dense(64 => 2000),        \u001b[90m# 130_000 parameters\u001b[39m\n",
       ") \u001b[90m        # Total: \u001b[39m146_832 parameters,\n",
       "\u001b[90m          #        plus \u001b[39m0 states."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_size = 64\n",
    "in_size = size(x)[1]\n",
    "out_size = size(y)[1]\n",
    "\n",
    "nn = Chain(\n",
    "    Dense(in_size => layer_size, tanh),\n",
    "    Dense(layer_size => layer_size, tanh),\n",
    "    Dense(layer_size => layer_size, tanh),\n",
    "    Dense(layer_size => layer_size, tanh),\n",
    "    Dense(layer_size => layer_size, tanh),\n",
    "    Dense(layer_size => out_size)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam(0.03, (0.9, 0.999), 1.0e-8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = Adam(0.03f0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MersenneTwister(12345)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = MersenneTwister()\n",
    "Random.seed!(rng, 12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mNo functional GPU backend found! Defaulting to CPU.\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m1. If no GPU is available, nothing needs to be done.\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m2. If GPU is available, load the corresponding trigger package.\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m    a. LuxCUDA.jl for NVIDIA CUDA Support.\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m    b. LuxAMDGPU.jl for AMD GPU ROCM Support.\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m    c. Metal.jl for Apple Metal GPU Support.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ LuxDeviceUtils ~/.julia/packages/LuxDeviceUtils/FNTXQ/src/LuxDeviceUtils.jl:212\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Lux.Experimental.TrainState{Chain{@NamedTuple{layer_1::Dense{true, typeof(tanh_fast), typeof(glorot_uniform), typeof(zeros32)}, layer_2::Dense{true, typeof(tanh_fast), typeof(glorot_uniform), typeof(zeros32)}, layer_3::Dense{true, typeof(tanh_fast), typeof(glorot_uniform), typeof(zeros32)}, layer_4::Dense{true, typeof(tanh_fast), typeof(glorot_uniform), typeof(zeros32)}, layer_5::Dense{true, typeof(tanh_fast), typeof(glorot_uniform), typeof(zeros32)}, layer_6::Dense{true, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}}, Nothing}, @NamedTuple{layer_1::@NamedTuple{weight::Matrix{Float32}, bias::Matrix{Float32}}, layer_2::@NamedTuple{weight::Matrix{Float32}, bias::Matrix{Float32}}, layer_3::@NamedTuple{weight::Matrix{Float32}, bias::Matrix{Float32}}, layer_4::@NamedTuple{weight::Matrix{Float32}, bias::Matrix{Float32}}, layer_5::@NamedTuple{weight::Matrix{Float32}, bias::Matrix{Float32}}, layer_6::@NamedTuple{weight::Matrix{Float32}, bias::Matrix{Float32}}}, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}, layer_5::@NamedTuple{}, layer_6::@NamedTuple{}}, @NamedTuple{layer_1::@NamedTuple{weight::Optimisers.Leaf{Adam, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Optimisers.Leaf{Adam, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}}, layer_2::@NamedTuple{weight::Optimisers.Leaf{Adam, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Optimisers.Leaf{Adam, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}}, layer_3::@NamedTuple{weight::Optimisers.Leaf{Adam, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Optimisers.Leaf{Adam, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}}, layer_4::@NamedTuple{weight::Optimisers.Leaf{Adam, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Optimisers.Leaf{Adam, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}}, layer_5::@NamedTuple{weight::Optimisers.Leaf{Adam, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Optimisers.Leaf{Adam, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}}, layer_6::@NamedTuple{weight::Optimisers.Leaf{Adam, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Optimisers.Leaf{Adam, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}}}}(Chain(), (layer_1 = (weight = Float32[0.11563481 0.018158812; 0.15257396 0.18455714; … ; 0.21209772 -0.21476188; -0.10581167 0.16260183], bias = Float32[0.0; 0.0; … ; 0.0; 0.0;;]), layer_2 = (weight = Float32[0.15995897 -0.14784293 … -0.17566215 0.18558906; -0.13907914 0.12702984 … 0.07183944 0.14951581; … ; 0.09902868 0.20652233 … -0.17267726 0.08181912; 0.032899145 -0.08194946 … 0.11063333 0.13348632], bias = Float32[0.0; 0.0; … ; 0.0; 0.0;;]), layer_3 = (weight = Float32[-0.045591414 -0.19919959 … -0.022852978 -0.18496148; 0.078357846 0.1675198 … -0.16953108 -0.026778717; … ; 0.19517773 -0.07542062 … 0.12153344 0.03950263; -0.18826294 -0.16741212 … -0.06935583 0.04550996], bias = Float32[0.0; 0.0; … ; 0.0; 0.0;;]), layer_4 = (weight = Float32[0.032530174 0.19099183 … -0.05264145 -0.014717493; -0.12587078 0.17158248 … 0.18662605 0.14319901; … ; -0.15302059 0.084780194 … 0.12668373 -0.028690483; 0.009236005 0.12340029 … -0.15555789 0.1285314], bias = Float32[0.0; 0.0; … ; 0.0; 0.0;;]), layer_5 = (weight = Float32[0.14315644 -0.11119123 … 0.06150079 -0.10029913; -0.09400583 -0.11514345 … 0.02104027 0.027403153; … ; 0.13041383 -0.11766737 … -0.17718114 -0.1589218; 0.058702003 -0.20390585 … -0.19419356 0.048939276], bias = Float32[0.0; 0.0; … ; 0.0; 0.0;;]), layer_6 = (weight = Float32[0.02925441 0.023230199 … -0.033118885 0.039722532; 0.013374113 0.03187107 … 0.015070595 0.019236896; … ; -0.030823207 -0.050274815 … 0.0475656 0.005602553; 0.018291408 -0.041377712 … 0.0290008 -0.049667407], bias = Float32[0.0; 0.0; … ; 0.0; 0.0;;])), (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple(), layer_5 = NamedTuple(), layer_6 = NamedTuple()), (layer_1 = (weight = \u001b[32mLeaf(Adam(0.03, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0 0.0; 0.0 0.0; … ; 0.0 0.0; 0.0 0.0], Float32[0.0 0.0; 0.0 0.0; … ; 0.0 0.0; 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(0.03, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0; 0.0; … ; 0.0; 0.0;;], Float32[0.0; 0.0; … ; 0.0; 0.0;;], (0.9, 0.999))\u001b[32m)\u001b[39m), layer_2 = (weight = \u001b[32mLeaf(Adam(0.03, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(0.03, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0; 0.0; … ; 0.0; 0.0;;], Float32[0.0; 0.0; … ; 0.0; 0.0;;], (0.9, 0.999))\u001b[32m)\u001b[39m), layer_3 = (weight = \u001b[32mLeaf(Adam(0.03, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(0.03, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0; 0.0; … ; 0.0; 0.0;;], Float32[0.0; 0.0; … ; 0.0; 0.0;;], (0.9, 0.999))\u001b[32m)\u001b[39m), layer_4 = (weight = \u001b[32mLeaf(Adam(0.03, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(0.03, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0; 0.0; … ; 0.0; 0.0;;], Float32[0.0; 0.0; … ; 0.0; 0.0;;], (0.9, 0.999))\u001b[32m)\u001b[39m), layer_5 = (weight = \u001b[32mLeaf(Adam(0.03, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(0.03, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0; 0.0; … ; 0.0; 0.0;;], Float32[0.0; 0.0; … ; 0.0; 0.0;;], (0.9, 0.999))\u001b[32m)\u001b[39m), layer_6 = (weight = \u001b[32mLeaf(Adam(0.03, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(0.03, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0; 0.0; … ; 0.0; 0.0;;], Float32[0.0; 0.0; … ; 0.0; 0.0;;], (0.9, 0.999))\u001b[32m)\u001b[39m)), 0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tstate = Lux.Training.TrainState(rng, nn, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADTypes.AutoZygote()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vjp_rule = Lux.Training.AutoZygote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss_function (generic function with 1 method)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss_function(model, ps, st, data)\n",
    "    y_pred, st = Lux.apply(model, data[1], ps, st)\n",
    "    mse_loss = mean(abs2, y_pred .- data[2])\n",
    "    return mse_loss, st, ()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\t|| Loss: 0.002832305816359235\n",
      "Epoch: 2\t|| Loss: 0.004794318909954356\n",
      "Epoch: 3\t|| Loss: 0.47515854605393987\n",
      "Epoch: 4\t|| Loss: 0.7340083808490466\n",
      "Epoch: 5\t|| Loss: 0.057161317368413386\n",
      "Epoch: 6\t|| Loss: 0.18432009675953223\n",
      "Epoch: 7\t|| Loss: 0.02877649975387729\n",
      "Epoch: 8\t|| Loss: 0.502462871224478\n",
      "Epoch: 9\t|| Loss: 0.33877269833085066\n",
      "Epoch: 10\t|| Loss: 0.1144766315830105\n",
      "Epoch: 11\t|| Loss: 0.07495077804376002\n",
      "Epoch: 12\t|| Loss: 0.035356634988113596\n",
      "Epoch: 13\t|| Loss: 0.11963525321315105\n",
      "Epoch: 14\t|| Loss: 0.048837769113728585\n",
      "Epoch: 15\t|| Loss: 0.025539591640547733\n",
      "Epoch: 16\t|| Loss: 0.05154489513452936\n",
      "Epoch: 17\t|| Loss: 0.053287166932634834\n",
      "Epoch: 18\t|| Loss: 0.018816666264048876\n",
      "Epoch: 19\t|| Loss: 0.029037616294111123\n",
      "Epoch: 20\t|| Loss: 0.038922209609557085\n",
      "Epoch: 21\t|| Loss: 0.020083722651660228\n",
      "Epoch: 22\t|| Loss: 0.021316762714488895\n",
      "Epoch: 23\t|| Loss: 0.02105280220738864\n",
      "Epoch: 24\t|| Loss: 0.022083059669362547\n",
      "Epoch: 25\t|| Loss: 0.013362293495350432\n",
      "Epoch: 26\t|| Loss: 0.014572387876409264\n",
      "Epoch: 27\t|| Loss: 0.016593919827984846\n",
      "Epoch: 28\t|| Loss: 0.010499749459075801\n",
      "Epoch: 29\t|| Loss: 0.011269463598244605\n",
      "Epoch: 30\t|| Loss: 0.010391508102196197\n",
      "Epoch: 31\t|| Loss: 0.00889418206727814\n",
      "Epoch: 32\t|| Loss: 0.008576477343659275\n",
      "Epoch: 33\t|| Loss: 0.006686498705535178\n",
      "Epoch: 34\t|| Loss: 0.0075540883537961664\n",
      "Epoch: 35\t|| Loss: 0.005897085787606358\n",
      "Epoch: 36\t|| Loss: 0.005058702553520486\n",
      "Epoch: 37\t|| Loss: 0.005488212173416334\n",
      "Epoch: 38\t|| Loss: 0.004162759431850388\n",
      "Epoch: 39\t|| Loss: 0.004381850238350082\n",
      "Epoch: 40\t|| Loss: 0.003517165015628192\n",
      "Epoch: 41\t|| Loss: 0.0033997502983477666\n",
      "Epoch: 42\t|| Loss: 0.0030268586510750453\n",
      "Epoch: 43\t|| Loss: 0.0026114080005926405\n",
      "Epoch: 44\t|| Loss: 0.0027251493208605205\n",
      "Epoch: 45\t|| Loss: 0.001977021014871837\n",
      "Epoch: 46\t|| Loss: 0.0022375996530258234\n",
      "Epoch: 47\t|| Loss: 0.001722839267393677\n",
      "Epoch: 48\t|| Loss: 0.0016812327877456642\n",
      "Epoch: 49\t|| Loss: 0.0014702776094574304\n",
      "Epoch: 50\t|| Loss: 0.0013799487204800163\n",
      "Epoch: 51\t|| Loss: 0.0012789009146787096\n",
      "Epoch: 52\t|| Loss: 0.0010309376281971046\n",
      "Epoch: 53\t|| Loss: 0.0010498155979041062\n",
      "Epoch: 54\t|| Loss: 0.0008757442641177816\n",
      "Epoch: 55\t|| Loss: 0.0008669802202443977\n",
      "Epoch: 56\t|| Loss: 0.0007103205860874915\n",
      "Epoch: 57\t|| Loss: 0.0007050474454773285\n",
      "Epoch: 58\t|| Loss: 0.0005693871998128879\n",
      "Epoch: 59\t|| Loss: 0.0006068678227158384\n",
      "Epoch: 60\t|| Loss: 0.0004424655722374372\n",
      "Epoch: 61\t|| Loss: 0.0004974739004327918\n",
      "Epoch: 62\t|| Loss: 0.0003772153893564417\n",
      "Epoch: 63\t|| Loss: 0.00040479436101386907\n",
      "Epoch: 64\t|| Loss: 0.00030062165177572876\n",
      "Epoch: 65\t|| Loss: 0.00032416597825937315\n",
      "Epoch: 66\t|| Loss: 0.00026353423150558315\n",
      "Epoch: 67\t|| Loss: 0.0002569859004911833\n",
      "Epoch: 68\t|| Loss: 0.00021750101911822376\n",
      "Epoch: 69\t|| Loss: 0.00020877231557667962\n",
      "Epoch: 70\t|| Loss: 0.00017745915585915946\n",
      "Epoch: 71\t|| Loss: 0.00016723888101065747\n",
      "Epoch: 72\t|| Loss: 0.00015515083722863942\n",
      "Epoch: 73\t|| Loss: 0.00012949230357131366\n",
      "Epoch: 74\t|| Loss: 0.00012870867203124844\n",
      "Epoch: 75\t|| Loss: 0.00010504072141934337\n",
      "Epoch: 76\t|| Loss: 0.00010616343122539405\n",
      "Epoch: 77\t|| Loss: 8.447249534317905e-5\n",
      "Epoch: 78\t|| Loss: 9.06124522787307e-5\n",
      "Epoch: 79\t|| Loss: 6.612656559451774e-5\n",
      "Epoch: 80\t|| Loss: 7.395056942533576e-5\n",
      "Epoch: 81\t|| Loss: 5.5335094648331996e-5\n",
      "Epoch: 82\t|| Loss: 5.8732937637558856e-5\n",
      "Epoch: 83\t|| Loss: 4.843401451477572e-5\n",
      "Epoch: 84\t|| Loss: 4.556133494763975e-5\n",
      "Epoch: 85\t|| Loss: 4.0838790638928924e-5\n",
      "Epoch: 86\t|| Loss: 3.545987335284655e-5\n",
      "Epoch: 87\t|| Loss: 3.546719096516491e-5\n",
      "Epoch: 88\t|| Loss: 2.8569551626763792e-5\n",
      "Epoch: 89\t|| Loss: 2.889554850086479e-5\n",
      "Epoch: 90\t|| Loss: 2.300139599290613e-5\n",
      "Epoch: 91\t|| Loss: 2.3696967609603195e-5\n",
      "Epoch: 92\t|| Loss: 1.989879638516281e-5\n",
      "Epoch: 93\t|| Loss: 1.7908670419907647e-5\n",
      "Epoch: 94\t|| Loss: 1.796884699231202e-5\n",
      "Epoch: 95\t|| Loss: 1.318487205714579e-5\n",
      "Epoch: 96\t|| Loss: 1.529366314893542e-5\n",
      "Epoch: 97\t|| Loss: 1.143212516364704e-5\n",
      "Epoch: 98\t|| Loss: 1.1728967737482783e-5\n",
      "Epoch: 99\t|| Loss: 9.988474191848099e-6\n",
      "Epoch: 100\t|| Loss: 9.054355994933927e-6\n",
      "Epoch: 101\t|| Loss: 8.81461883218543e-6\n",
      "Epoch: 102\t|| Loss: 7.017399680501947e-6\n",
      "Epoch: 103\t|| Loss: 7.5866731820063824e-6\n",
      "Epoch: 104\t|| Loss: 5.728702265906101e-6\n",
      "Epoch: 105\t|| Loss: 5.911353138149576e-6\n",
      "Epoch: 106\t|| Loss: 5.3552372603854805e-6\n",
      "Epoch: 107\t|| Loss: 4.342117115436014e-6\n",
      "Epoch: 108\t|| Loss: 4.718650214509385e-6\n",
      "Epoch: 109\t|| Loss: 3.6716256431440118e-6\n",
      "Epoch: 110\t|| Loss: 3.5710814782887505e-6\n",
      "Epoch: 111\t|| Loss: 3.4625887765978063e-6\n",
      "Epoch: 112\t|| Loss: 2.7231485949962074e-6\n",
      "Epoch: 113\t|| Loss: 3.012363253359365e-6\n",
      "Epoch: 114\t|| Loss: 2.3189997939125104e-6\n",
      "Epoch: 115\t|| Loss: 2.2829048995378785e-6\n",
      "Epoch: 116\t|| Loss: 2.2881680672168792e-6\n",
      "Epoch: 117\t|| Loss: 1.7482145649779107e-6\n",
      "Epoch: 118\t|| Loss: 1.8971874100829132e-6\n",
      "Epoch: 119\t|| Loss: 1.6276560683607877e-6\n",
      "Epoch: 120\t|| Loss: 1.4626059346780341e-6\n",
      "Epoch: 121\t|| Loss: 1.5133982935597645e-6\n",
      "Epoch: 122\t|| Loss: 1.2275788857412483e-6\n",
      "Epoch: 123\t|| Loss: 1.2254112728164135e-6\n",
      "Epoch: 124\t|| Loss: 1.1732682348629909e-6\n",
      "Epoch: 125\t|| Loss: 9.95760571226725e-7\n",
      "Epoch: 126\t|| Loss: 1.0339444726682198e-6\n",
      "Epoch: 127\t|| Loss: 9.160201948073321e-7\n",
      "Epoch: 128\t|| Loss: 8.373851161791566e-7\n",
      "Epoch: 129\t|| Loss: 8.608150171623751e-7\n",
      "Epoch: 130\t|| Loss: 7.575948855414264e-7\n",
      "Epoch: 131\t|| Loss: 7.074163397311187e-7\n",
      "Epoch: 132\t|| Loss: 7.393563656911757e-7\n",
      "Epoch: 133\t|| Loss: 6.224246447715894e-7\n",
      "Epoch: 134\t|| Loss: 6.348061563204373e-7\n",
      "Epoch: 135\t|| Loss: 6.249409709273499e-7\n",
      "Epoch: 136\t|| Loss: 5.492816074073357e-7\n",
      "Epoch: 137\t|| Loss: 5.596499790866272e-7\n",
      "Epoch: 138\t|| Loss: 5.461437963658608e-7\n",
      "Epoch: 139\t|| Loss: 4.970223189261643e-7\n",
      "Epoch: 140\t|| Loss: 5.026307579657617e-7\n",
      "Epoch: 141\t|| Loss: 4.900418620831453e-7\n",
      "Epoch: 142\t|| Loss: 4.524782227216824e-7\n",
      "Epoch: 143\t|| Loss: 4.6301753824320835e-7\n",
      "Epoch: 144\t|| Loss: 4.4824546129098086e-7\n",
      "Epoch: 145\t|| Loss: 4.2267336862747666e-7\n",
      "Epoch: 146\t|| Loss: 4.2605471090756993e-7\n",
      "Epoch: 147\t|| Loss: 4.2215466726828196e-7\n",
      "Epoch: 148\t|| Loss: 3.9807526515342764e-7\n",
      "Epoch: 149\t|| Loss: 4.0130320576009413e-7\n",
      "Epoch: 150\t|| Loss: 3.9861666134510914e-7\n",
      "Epoch: 151\t|| Loss: 3.815986409944778e-7\n",
      "Epoch: 152\t|| Loss: 3.82176925329849e-7\n",
      "Epoch: 153\t|| Loss: 3.8081630413834195e-7\n",
      "Epoch: 154\t|| Loss: 3.697407109118218e-7\n",
      "Epoch: 155\t|| Loss: 3.6605603647839087e-7\n",
      "Epoch: 156\t|| Loss: 3.682697280202189e-7\n",
      "Epoch: 157\t|| Loss: 3.601720347795924e-7\n",
      "Epoch: 158\t|| Loss: 3.547422816608209e-7\n",
      "Epoch: 159\t|| Loss: 3.571325737981659e-7\n",
      "Epoch: 160\t|| Loss: 3.527236111777895e-7\n",
      "Epoch: 161\t|| Loss: 3.4741828598423825e-7\n",
      "Epoch: 162\t|| Loss: 3.47340643129576e-7\n",
      "Epoch: 163\t|| Loss: 3.466528419448606e-7\n",
      "Epoch: 164\t|| Loss: 3.4184856748643995e-7\n",
      "Epoch: 165\t|| Loss: 3.404044112924198e-7\n",
      "Epoch: 166\t|| Loss: 3.4042270005078463e-7\n",
      "Epoch: 167\t|| Loss: 3.3800649757437505e-7\n",
      "Epoch: 168\t|| Loss: 3.3511780077716375e-7\n",
      "Epoch: 169\t|| Loss: 3.350279994560533e-7\n",
      "Epoch: 170\t|| Loss: 3.3427208663121667e-7\n",
      "Epoch: 171\t|| Loss: 3.3170270296207166e-7\n",
      "Epoch: 172\t|| Loss: 3.304630373262716e-7\n",
      "Epoch: 173\t|| Loss: 3.303235345448781e-7\n",
      "Epoch: 174\t|| Loss: 3.29214928329922e-7\n",
      "Epoch: 175\t|| Loss: 3.2712961036366767e-7\n",
      "Epoch: 176\t|| Loss: 3.266547480946928e-7\n",
      "Epoch: 177\t|| Loss: 3.262957067873718e-7\n",
      "Epoch: 178\t|| Loss: 3.2502043601234585e-7\n",
      "Epoch: 179\t|| Loss: 3.237229275123697e-7\n",
      "Epoch: 180\t|| Loss: 3.23183733938641e-7\n",
      "Epoch: 181\t|| Loss: 3.228362813274306e-7\n",
      "Epoch: 182\t|| Loss: 3.216808017462168e-7\n",
      "Epoch: 183\t|| Loss: 3.20685245612317e-7\n",
      "Epoch: 184\t|| Loss: 3.202060670352473e-7\n",
      "Epoch: 185\t|| Loss: 3.197161275816346e-7\n",
      "Epoch: 186\t|| Loss: 3.188935814085177e-7\n",
      "Epoch: 187\t|| Loss: 3.1794181737325817e-7\n",
      "Epoch: 188\t|| Loss: 3.1748259698550087e-7\n",
      "Epoch: 189\t|| Loss: 3.170080570654428e-7\n",
      "Epoch: 190\t|| Loss: 3.1628220753787893e-7\n",
      "Epoch: 191\t|| Loss: 3.1554313799125283e-7\n",
      "Epoch: 192\t|| Loss: 3.1493006388039127e-7\n",
      "Epoch: 193\t|| Loss: 3.144966455510586e-7\n",
      "Epoch: 194\t|| Loss: 3.1393115204227505e-7\n",
      "Epoch: 195\t|| Loss: 3.132248320703637e-7\n",
      "Epoch: 196\t|| Loss: 3.1262647126116103e-7\n",
      "Epoch: 197\t|| Loss: 3.1214037294531986e-7\n",
      "Epoch: 198\t|| Loss: 3.1164895279463027e-7\n",
      "Epoch: 199\t|| Loss: 3.110646669410385e-7\n",
      "Epoch: 200\t|| Loss: 3.1045705560468046e-7\n",
      "Epoch: 201\t|| Loss: 3.099278604915865e-7\n",
      "Epoch: 202\t|| Loss: 3.094512277726799e-7\n",
      "Epoch: 203\t|| Loss: 3.0894206733938465e-7\n",
      "Epoch: 204\t|| Loss: 3.0841160591206845e-7\n",
      "Epoch: 205\t|| Loss: 3.0784658525383295e-7\n",
      "Epoch: 206\t|| Loss: 3.0733707998389716e-7\n",
      "Epoch: 207\t|| Loss: 3.068665035344275e-7\n",
      "Epoch: 208\t|| Loss: 3.063844806671802e-7\n",
      "Epoch: 209\t|| Loss: 3.058634597901619e-7\n",
      "Epoch: 210\t|| Loss: 3.053507929611358e-7\n",
      "Epoch: 211\t|| Loss: 3.048469146934516e-7\n",
      "Epoch: 212\t|| Loss: 3.0437607349467473e-7\n",
      "Epoch: 213\t|| Loss: 3.039036083654549e-7\n",
      "Epoch: 214\t|| Loss: 3.0342556890493964e-7\n",
      "Epoch: 215\t|| Loss: 3.029282915982272e-7\n",
      "Epoch: 216\t|| Loss: 3.0244401921961835e-7\n",
      "Epoch: 217\t|| Loss: 3.019645113549542e-7\n",
      "Epoch: 218\t|| Loss: 3.0150522379799047e-7\n",
      "Epoch: 219\t|| Loss: 3.010431790601108e-7\n",
      "Epoch: 220\t|| Loss: 3.0057695897302247e-7\n",
      "Epoch: 221\t|| Loss: 3.0010796725750023e-7\n",
      "Epoch: 222\t|| Loss: 2.996382779538696e-7\n",
      "Epoch: 223\t|| Loss: 2.9917507134340513e-7\n",
      "Epoch: 224\t|| Loss: 2.987216895416637e-7\n",
      "Epoch: 225\t|| Loss: 2.982711334592732e-7\n",
      "Epoch: 226\t|| Loss: 2.9782205451410637e-7\n",
      "Epoch: 227\t|| Loss: 2.9737181702550834e-7\n",
      "Epoch: 228\t|| Loss: 2.969204155065644e-7\n",
      "Epoch: 229\t|| Loss: 2.9647048019078226e-7\n",
      "Epoch: 230\t|| Loss: 2.960240597473142e-7\n",
      "Epoch: 231\t|| Loss: 2.955818856102667e-7\n",
      "Epoch: 232\t|| Loss: 2.9514251665186915e-7\n",
      "Epoch: 233\t|| Loss: 2.9470784836471116e-7\n",
      "Epoch: 234\t|| Loss: 2.942728043091657e-7\n",
      "Epoch: 235\t|| Loss: 2.938396851962523e-7\n",
      "Epoch: 236\t|| Loss: 2.934070902842411e-7\n",
      "Epoch: 237\t|| Loss: 2.9297610052367696e-7\n",
      "Epoch: 238\t|| Loss: 2.92546056354924e-7\n",
      "Epoch: 239\t|| Loss: 2.9211881333341327e-7\n",
      "Epoch: 240\t|| Loss: 2.9169328286008274e-7\n",
      "Epoch: 241\t|| Loss: 2.9127026141733825e-7\n",
      "Epoch: 242\t|| Loss: 2.908497506554241e-7\n",
      "Epoch: 243\t|| Loss: 2.904311253874772e-7\n",
      "Epoch: 244\t|| Loss: 2.90014369767994e-7\n",
      "Epoch: 245\t|| Loss: 2.895998510291516e-7\n",
      "Epoch: 246\t|| Loss: 2.891871330356984e-7\n",
      "Epoch: 247\t|| Loss: 2.887761038522113e-7\n",
      "Epoch: 248\t|| Loss: 2.883670310812121e-7\n",
      "Epoch: 249\t|| Loss: 2.879599267710102e-7\n",
      "Epoch: 250\t|| Loss: 2.8755480445283616e-7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2000×100 Matrix{Float64}:\n",
       " -0.000147648  -6.3337e-5     1.56105e-5   …   6.96519e-5    0.000125582\n",
       "  0.000247425   0.000156936   6.84495e-5      -0.000177517  -0.000260514\n",
       " -0.0002891    -0.000128328   2.71618e-5       0.000156032   0.000291043\n",
       " -0.000400779  -0.000223564  -5.20148e-5       0.000251393   0.000400971\n",
       " -0.000145626  -0.000123345  -9.93497e-5       0.000113251   0.000142835\n",
       " -0.000248291  -0.00011888    6.64057e-6   …   0.000141498   0.000253597\n",
       "  0.000312208   0.000194239   7.90641e-5      -0.00021064   -0.000314882\n",
       "  0.00013148    2.71912e-5   -7.21439e-5      -5.93596e-5   -0.000142195\n",
       "  2.05918e-6   -3.61742e-5   -7.14286e-5       2.25224e-5   -1.58277e-6\n",
       "  0.000493756   0.000291875   9.25302e-5      -0.000316979  -0.00050764\n",
       "  0.000118045   0.000153113   0.000184613  …  -0.000116544  -9.33197e-5\n",
       " -0.000474337  -0.000305706  -0.000141166      0.000315813   0.000461859\n",
       " -0.000180209  -8.25657e-5    1.06679e-5       0.000108375   0.000185729\n",
       "  ⋮                                        ⋱                \n",
       "  0.169354      0.169233      0.169114         0.169145      0.169033\n",
       "  0.145077      0.145064      0.145053         0.145349      0.14534\n",
       "  0.126012      0.125943      0.125875     …   0.125934      0.125873\n",
       "  0.109799      0.109938      0.110076         0.11059       0.110719\n",
       "  0.0978865     0.0976728     0.0974625        0.0969735     0.0967792\n",
       "  0.0864427     0.0864374     0.0864338        0.0866434     0.0866462\n",
       "  0.0775782     0.0775361     0.0774936        0.0773843     0.0773421\n",
       "  0.0695585     0.0695747     0.0695915    …   0.069899      0.0699181\n",
       "  0.0632506     0.0632216     0.0631918        0.063027      0.0629957\n",
       "  0.0571856     0.0572328     0.0572814        0.057579      0.0576332\n",
       "  0.0523356     0.0523495     0.0523639        0.0525285     0.0525458\n",
       "  0.0480998     0.0481176     0.0481364        0.0480748     0.0480986"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function main(tstate::Lux.Experimental.TrainState, vjp, data, epochs)\n",
    "    # data = data .|> gpu_device()\n",
    "    for epoch in 1:epochs\n",
    "        grads, loss, stats, tstate = Lux.Training.compute_gradients(\n",
    "            vjp, loss_function, data, tstate)\n",
    "        println(\"Epoch: $(epoch)\\t|| Loss: $(loss)\")\n",
    "        tstate = Lux.Training.apply_gradients(tstate, grads)\n",
    "    end\n",
    "    return tstate\n",
    "end\n",
    "\n",
    "dev_cpu = cpu_device()\n",
    "\n",
    "tstate = main(tstate, vjp_rule, (x, y), 250)\n",
    "y_pred = dev_cpu(Lux.apply(tstate.model, x, tstate.parameters, tstate.states)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "DimensionMismatch: A has dimensions (64,2) but B has dimensions (100,2)",
     "output_type": "error",
     "traceback": [
      "DimensionMismatch: A has dimensions (64,2) but B has dimensions (100,2)",
      "",
      "Stacktrace:",
      "  [1] gemm_wrapper!(C::Matrix{Float64}, tA::Char, tB::Char, A::Matrix{Float64}, B::Matrix{Float64}, _add::LinearAlgebra.MulAddMul{true, true, Bool, Bool})",
      "    @ LinearAlgebra /Applications/Julia-1.10.app/Contents/Resources/julia/share/julia/stdlib/v1.10/LinearAlgebra/src/matmul.jl:577",
      "  [2] generic_matmatmul!",
      "    @ /Applications/Julia-1.10.app/Contents/Resources/julia/share/julia/stdlib/v1.10/LinearAlgebra/src/matmul.jl:352 [inlined]",
      "  [3] mul!",
      "    @ /Applications/Julia-1.10.app/Contents/Resources/julia/share/julia/stdlib/v1.10/LinearAlgebra/src/matmul.jl:263 [inlined]",
      "  [4] mul!",
      "    @ /Applications/Julia-1.10.app/Contents/Resources/julia/share/julia/stdlib/v1.10/LinearAlgebra/src/matmul.jl:237 [inlined]",
      "  [5] *(A::Matrix{Float32}, B::Matrix{Float64})",
      "    @ LinearAlgebra /Applications/Julia-1.10.app/Contents/Resources/julia/share/julia/stdlib/v1.10/LinearAlgebra/src/matmul.jl:113",
      "  [6] Dense",
      "    @ ~/.julia/packages/Lux/ufsdt/src/layers/basic.jl:234 [inlined]",
      "  [7] apply(model::Dense{true, typeof(tanh_fast), typeof(glorot_uniform), typeof(zeros32)}, x::Matrix{Float64}, ps::@NamedTuple{weight::Matrix{Float32}, bias::Matrix{Float32}}, st::@NamedTuple{})",
      "    @ LuxCore ~/.julia/packages/LuxCore/t4mG0/src/LuxCore.jl:164",
      "  [8] macro expansion",
      "    @ ~/.julia/packages/Lux/ufsdt/src/layers/containers.jl:0 [inlined]",
      "  [9] applychain(layers::@NamedTuple{layer_1::Dense{true, typeof(tanh_fast), typeof(glorot_uniform), typeof(zeros32)}, layer_2::Dense{true, typeof(tanh_fast), typeof(glorot_uniform), typeof(zeros32)}, layer_3::Dense{true, typeof(tanh_fast), typeof(glorot_uniform), typeof(zeros32)}, layer_4::Dense{true, typeof(tanh_fast), typeof(glorot_uniform), typeof(zeros32)}, layer_5::Dense{true, typeof(tanh_fast), typeof(glorot_uniform), typeof(zeros32)}, layer_6::Dense{true, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}}, x::Matrix{Float64}, ps::@NamedTuple{layer_1::@NamedTuple{weight::Matrix{Float32}, bias::Matrix{Float32}}, layer_2::@NamedTuple{weight::Matrix{Float32}, bias::Matrix{Float32}}, layer_3::@NamedTuple{weight::Matrix{Float32}, bias::Matrix{Float32}}, layer_4::@NamedTuple{weight::Matrix{Float32}, bias::Matrix{Float32}}, layer_5::@NamedTuple{weight::Matrix{Float32}, bias::Matrix{Float32}}, layer_6::@NamedTuple{weight::Matrix{Float32}, bias::Matrix{Float32}}}, st::@NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}, layer_5::@NamedTuple{}, layer_6::@NamedTuple{}})",
      "    @ Lux ~/.julia/packages/Lux/ufsdt/src/layers/containers.jl:481",
      " [10] Chain",
      "    @ ~/.julia/packages/Lux/ufsdt/src/layers/containers.jl:479 [inlined]",
      " [11] apply(model::Chain{@NamedTuple{layer_1::Dense{true, typeof(tanh_fast), typeof(glorot_uniform), typeof(zeros32)}, layer_2::Dense{true, typeof(tanh_fast), typeof(glorot_uniform), typeof(zeros32)}, layer_3::Dense{true, typeof(tanh_fast), typeof(glorot_uniform), typeof(zeros32)}, layer_4::Dense{true, typeof(tanh_fast), typeof(glorot_uniform), typeof(zeros32)}, layer_5::Dense{true, typeof(tanh_fast), typeof(glorot_uniform), typeof(zeros32)}, layer_6::Dense{true, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}}, Nothing}, x::Matrix{Float64}, ps::@NamedTuple{layer_1::@NamedTuple{weight::Matrix{Float32}, bias::Matrix{Float32}}, layer_2::@NamedTuple{weight::Matrix{Float32}, bias::Matrix{Float32}}, layer_3::@NamedTuple{weight::Matrix{Float32}, bias::Matrix{Float32}}, layer_4::@NamedTuple{weight::Matrix{Float32}, bias::Matrix{Float32}}, layer_5::@NamedTuple{weight::Matrix{Float32}, bias::Matrix{Float32}}, layer_6::@NamedTuple{weight::Matrix{Float32}, bias::Matrix{Float32}}}, st::@NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}, layer_5::@NamedTuple{}, layer_6::@NamedTuple{}})",
      "    @ LuxCore ~/.julia/packages/LuxCore/t4mG0/src/LuxCore.jl:164",
      " [12] top-level scope",
      "    @ In[20]:2"
     ]
    }
   ],
   "source": [
    "ps, st = Lux.setup(rng, nn)\n",
    "y_pred, st = Lux.apply(nn, input, ps, st)\n",
    "\n",
    "print(size(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 2000)\n"
     ]
    }
   ],
   "source": [
    "input = npzread(\"../files/train_in_emul_\" * string(nc) * \".npy\")\n",
    "output = npzread(\"../files/train_out_emul_\" * string(nc) * \".npy\")\n",
    "\n",
    "pkz = npzread(\"../files/train_pkz_emul_\" * string(nc) * \".npy\")\n",
    "mu2 = npzread(\"../files/train_mu2_emul_\" * string(nc) * \".npy\")\n",
    "prim = npzread(\"../files/train_prim_emul_\" * string(nc) * \".npy\")\n",
    "tf2 = npzread(\"../files/train_tf2_emul_\" * string(nc) * \".npy\")\n",
    "\n",
    "println(size(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000×2000 Matrix{Float64}:\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " ⋮                        ⋮              ⋱            ⋮                   \n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prova = pkz./(prim .* tf2 .* mu2)\n",
    "prova = np.reshape(prova, [nc,nk*nz])\n",
    "\n",
    "prova-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearAlgebra.Adjoint{Float64, Matrix{Float64}}"
     ]
    }
   ],
   "source": [
    "input = npzread(\"../files/train_in_emul_\" * string(nc) * \".npy\")\n",
    "output = npzread(\"../files/train_pkz_emul_\" * string(nc) * \".npy\")\n",
    "\n",
    "x = input\n",
    "y = np.reshape(output, [nc,nk*nz])'\n",
    "\n",
    "print(typeof(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia_env nt8 1.10.2",
   "language": "julia",
   "name": "julia_env-nt8-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
